{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRujuWkuWrOc"
      },
      "source": [
        "# Introduction and Setup\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 280
        },
        "id": "ROl9l_y1WiNO",
        "outputId": "e173aeca-9e1f-4127-bb93-1212f76d045f"
      },
      "outputs": [],
      "source": [
        "# Import necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "\n",
        "# Device configuration (use GPU if available)\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "# Load the CIFAR-10 dataset (you can also use Fashion MNIST if preferred)\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))  # Normalize to [-1, 1]\n",
        "])\n",
        "\n",
        "# Load the CIFAR-10 dataset from the local 'data' folder\n",
        "train_dataset = torchvision.datasets.CIFAR10(root='./data', train=True, download=False, transform=transform)\n",
        "test_dataset = torchvision.datasets.CIFAR10(root='./data', train=False, download=False, transform=transform)\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Visualize some sample images from the dataset\n",
        "classes = ('plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "def imshow(img):\n",
        "    img = img / 2 + 0.5  # Unnormalize the image\n",
        "    npimg = img.numpy()\n",
        "    plt.imshow(np.transpose(npimg, (1, 2, 0)))\n",
        "    plt.show()\n",
        "\n",
        "# Get some random training images\n",
        "dataiter = iter(train_loader)\n",
        "images, labels = next(dataiter)\n",
        "\n",
        "# Show images\n",
        "imshow(torchvision.utils.make_grid(images[:4]))\n",
        "print(' '.join(f'{classes[labels[j]]}' for j in range(4)))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oRNF7WgtWqyZ"
      },
      "source": [
        "# Building and Training an MLP\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-bVW3Sh4Wm28",
        "outputId": "0f508043-b59e-476b-e656-cc24204dee35"
      },
      "outputs": [],
      "source": [
        "# Define an MLP architecture for CIFAR-10\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)  # Flatten the 32x32x3 image to a vector\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 128)\n",
        "        self.fc4 = nn.Linear(128, 10)  # 10 classes for CIFAR-10\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)  # Flatten the input\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = torch.relu(self.fc3(x))\n",
        "        x = self.fc4(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the MLP model, define the loss function and the optimizer\n",
        "mlp_model = MLP().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(mlp_model.parameters(), lr=0.001)\n",
        "\n",
        "# Training the MLP model\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = mlp_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward pass and optimization\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluate the MLP model on the test set\n",
        "def evaluate(model, test_loader):\n",
        "    model.eval()  # Set the model to evaluation mode\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():  # No need to compute gradients during evaluation\n",
        "        for images, labels in test_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            outputs = model(images)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    return 100 * correct / total\n",
        "\n",
        "mlp_accuracy = evaluate(mlp_model, test_loader)\n",
        "print(f'Accuracy of MLP on CIFAR-10 test images: {mlp_accuracy:.2f}%')\n",
        "\n",
        "# Explain why MLP struggles with image data\n",
        "print(\"Note: MLPs struggle with image data because they don't capture spatial patterns like CNNs.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1kW44Dg5XHTE"
      },
      "source": [
        "# Building and Training a CNN\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ivlfA9WRW-0c",
        "outputId": "80593f10-80e9-4e5d-eecd-97dab6d78c7a"
      },
      "outputs": [],
      "source": [
        "# Define a simple CNN architecture for CIFAR-10\n",
        "class SimpleCNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN, self).__init__()\n",
        "        # Convolutional layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # 3 input channels (RGB), 32 filters\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Max pooling with a 2x2 kernel\n",
        "        # Fully connected layers\n",
        "        self.fc1 = nn.Linear(64 * 8 * 8, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)  # 10 output classes\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)  # Flatten the tensor\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the CNN model\n",
        "cnn_model = SimpleCNN().to(device)\n",
        "optimizer = optim.Adam(cnn_model.parameters(), lr=0.001)\n",
        "\n",
        "# Train the CNN model\n",
        "for epoch in range(num_epochs):\n",
        "    cnn_model.train()  # Set the model to training mode\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = cnn_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Evaluate the CNN model on the test set\n",
        "cnn_accuracy = evaluate(cnn_model, test_loader)\n",
        "print(f'Accuracy of CNN on CIFAR-10 test images: {cnn_accuracy:.2f}%')\n",
        "\n",
        "# Compare CNN vs. MLP performance\n",
        "print(f'Comparison: MLP accuracy = {mlp_accuracy:.2f}%, CNN accuracy = {cnn_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fUE5lb9dZHeL"
      },
      "source": [
        "# Calculating Input Size for the Fully Connected Layer\n",
        "To calculate the input size for the fully connected layer (self.fc1 = nn.Linear(64 * 8 * 8, 128)), you need to determine the dimensions of the output after the convolutional and pooling layers. Let’s walk through the calculation process step-by-step for the SimpleCNN architecture:\n",
        "\n",
        "Input Image Size:\n",
        "\n",
        "CIFAR-10 images are 32x32 pixels with 3 color channels (RGB). Thus, the input size is (3, 32, 32).\n",
        "\n",
        "## First Convolutional Layer (self.conv1):\n",
        "\n",
        "This layer has 32 filters (output channels) and a 3x3 kernel size, with padding of 1.\n",
        "\n",
        "Formula for output size after a convolutional layer:\n",
        "\n",
        "Output Height/Width =(Input Size−Kernel Size+2×Padding)/Stride + 1\n",
        "\n",
        "Applying the formula:\n",
        "Output Height= (32−3+2×1)/1+1= 32\n",
        "\n",
        "The output size remains (32, 32), and the number of channels is now 32.\n",
        "\n",
        "## First Pooling Layer (self.pool):\n",
        "\n",
        "This layer uses 2x2 max pooling with a stride of 2.\n",
        "Formula for output size after pooling:\n",
        "Output Size = Input Size / Pool Size\n",
        "\n",
        "Applying the formula:\n",
        "\n",
        "Output Height/Width= 32/2 =16\n",
        "\n",
        "The output size is now (32, 16, 16).\n",
        "\n",
        "## Second Convolutional Layer (self.conv2):\n",
        "\n",
        "This layer has 64 filters and uses a 3x3 kernel size with padding of 1.\n",
        "Applying the convolution formula again:\n",
        "\n",
        "Output Height = (16−3+2×1)/1+1=16\n",
        "\n",
        "The output size remains (64, 16, 16), as the number of channels is now 64.\n",
        "\n",
        "## Second Pooling Layer (self.pool):\n",
        "\n",
        "This layer uses the same 2x2 max pooling with a stride of 2.\n",
        "Applying the pooling formula:\n",
        "\n",
        "Output Height/Width =16/2=8\n",
        "\n",
        "The output size is now (64, 8, 8).\n",
        "\n",
        "## Fully Connected Layer Input Size\n",
        "After the convolutional and pooling layers, the output size is (64, 8, 8):\n",
        "\n",
        "### 64: Number of channels (from the second convolutional layer).\n",
        "### 8x8: Height and width of the feature map after the second pooling layer.\n",
        "\n",
        "To connect this output to a fully connected layer (nn.Linear), we need to flatten the feature map. The number of input features is:\n",
        "\n",
        "Input Features=64×8×8=4096\n",
        "\n",
        "So, the input size for the fully connected layer is 4096"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Fgz_7u4bXcmm"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "* The MLP flattens the image data and processes it without considering the spatial structure, leading to poor performance on image datasets like CIFAR-10.\n",
        "* The CNN utilizes convolutional layers that apply filters to the image, capturing spatial patterns and local features effectively. This structure allows CNNs to learn more meaningful features for tasks like image classification, resulting in higher accuracy."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BlKCl6_Va4qR",
        "outputId": "3f3d5eb2-bd82-4b99-f737-b51eed2f249b"
      },
      "outputs": [],
      "source": [
        "from torchinfo import summary\n",
        "\n",
        "# Define your model (e.g., SimpleCNN)\n",
        "class SimpleCNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SimpleCNN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(2, 2)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool(torch.relu(self.conv1(x)))\n",
        "        x = self.pool(torch.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 8 * 8)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model\n",
        "model = SimpleCNN2()\n",
        "\n",
        "summary(model, input_size=(1, 3, 32, 32))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzvgIvzdbl1q"
      },
      "source": [
        "In PyTorch, x.view() is used to reshape a tensor without changing its data. It’s a very commonly used method when you need to adjust the dimensions of a tensor to match the expected input size of another layer, particularly when transitioning between convolutional layers and fully connected (linear) layers in a neural network.\n",
        "\n",
        "The view() method returns a new tensor with the same data as the original tensor but with a different shape. The number of elements must remain the same after reshaping, so the total size of the tensor before and after the operation must match.\n",
        "\n",
        "x = x.view(shape)\n",
        "\n",
        "shape: A tuple that defines the new shape of the tensor. It must be compatible with the number of elements in the original tensor.\n",
        "One of the dimensions can be -1, which tells PyTorch to automatically infer the size of that dimension based on the remaining dimensions and the number of elements in the tensor.\n",
        "\n",
        "## Common Use Case: Flattening the Tensor\n",
        "\n",
        "A common scenario where view() is used is in transitioning between convolutional layers and fully connected (linear) layers in a neural network. Convolutional layers output 4-dimensional tensors (batch size, channels, height, width), while fully connected layers expect a 2-dimensional input (batch size, features). You use view() to flatten the tensor before passing it to the fully connected layer.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6MSrsu0blgY",
        "outputId": "5740ee38-e95e-4a6b-a3ce-97921bc9c6a4"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "# Example tensor from a convolutional layer: batch size = 1, 64 channels, 8x8 feature map\n",
        "x = torch.randn(1, 64, 8, 8)\n",
        "\n",
        "# Flatten the tensor to (batch_size, all_features)\n",
        "x = x.view(1, -1)  # Equivalent to x.view(1, 64 * 8 * 8)\n",
        "\n",
        "# Output shape\n",
        "print(x.shape)  # Output: torch.Size([1, 4096])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e60lwktUYmQg"
      },
      "source": [
        "# Regularization Techniques\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "63BrozCVXY0y"
      },
      "outputs": [],
      "source": [
        "# Apply dropout to MLP\n",
        "class MLPWithDropout(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPWithDropout, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = self.dropout(x)  # Dropout layer\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.dropout(x)\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Re-train the MLP with dropout and evaluate performance\n",
        "mlp_dropout_model = MLPWithDropout().to(device)\n",
        "optimizer = optim.Adam(mlp_dropout_model.parameters(), lr=0.001)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    mlp_dropout_model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "        optimizer.zero_grad()\n",
        "        outputs = mlp_dropout_model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "mlp_dropout_accuracy = evaluate(mlp_dropout_model, test_loader)\n",
        "print(f'MLP with Dropout Accuracy: {mlp_dropout_accuracy:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LplBCzDGcHOF"
      },
      "source": [
        "# MLP with Batch Normalization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "bfWlzvT8cGT5"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "# Define an MLP with Batch Normalization\n",
        "class MLPWithBatchNorm(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(MLPWithBatchNorm, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.bn1 = nn.BatchNorm1d(512)  # BatchNorm after first fully connected layer\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.bn2 = nn.BatchNorm1d(256)  # BatchNorm after second fully connected layer\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = torch.relu(self.bn1(self.fc1(x)))\n",
        "        x = torch.relu(self.bn2(self.fc2(x)))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the model, define loss function and optimizer\n",
        "model_bn = MLPWithBatchNorm().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer_bn = optim.Adam(model_bn.parameters(), lr=0.001)\n",
        "\n",
        "# Training function train(model, optimizer)\n",
        "def train(model, optimizer, num_epochs=10):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_N-hjA_rcNEG"
      },
      "source": [
        "#Adding L2 Regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "BkvwjWyAcKIS",
        "outputId": "03b491f9-a1db-4753-f509-59951699b018"
      },
      "outputs": [],
      "source": [
        "# Define an MLP without batch normalization for comparison\n",
        "class BasicMLP(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(BasicMLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(3 * 32 * 32, 512)\n",
        "        self.fc2 = nn.Linear(512, 256)\n",
        "        self.fc3 = nn.Linear(256, 10)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 3 * 32 * 32)\n",
        "        x = torch.relu(self.fc1(x))\n",
        "        x = torch.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "# Instantiate the basic MLP model\n",
        "model_l2 = BasicMLP().to(device)\n",
        "\n",
        "# Use L2 regularization by setting weight_decay parameter in the optimizer\n",
        "optimizer_l2 = optim.Adam(model_l2.parameters(), lr=0.001, weight_decay=0.01)  # L2 regularization\n",
        "\n",
        "# Train the MLP with L2 regularization\n",
        "train(model_l2, optimizer_l2)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_l2 = evaluate(model_l2, test_loader)\n",
        "print(f'MLP with L2 Regularization Accuracy: {accuracy_l2:.2f}%')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "StUgGw4ocaJJ"
      },
      "source": [
        "#Adding L1 Regularization\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FNCdPXuAcQgn"
      },
      "outputs": [],
      "source": [
        "# Define a function for L1 regularization\n",
        "def l1_regularization(model, lambda_l1):\n",
        "    l1_norm = sum(param.abs().sum() for param in model.parameters())\n",
        "    return lambda_l1 * l1_norm\n",
        "\n",
        "# Train function with L1 regularization\n",
        "def train_with_l1(model, optimizer, lambda_l1, num_epochs=5):\n",
        "    for epoch in range(num_epochs):\n",
        "        model.train()\n",
        "        running_loss = 0.0\n",
        "        for images, labels in train_loader:\n",
        "            images, labels = images.to(device), labels.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(images)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Add L1 regularization to the loss\n",
        "            l1_penalty = l1_regularization(model, lambda_l1)\n",
        "            loss += l1_penalty\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item()\n",
        "        print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}')\n",
        "\n",
        "# Instantiate another model and optimizer\n",
        "model_l1 = BasicMLP().to(device)\n",
        "optimizer_l1 = optim.Adam(model_l1.parameters(), lr=0.001)\n",
        "\n",
        "# Train the MLP with L1 regularization\n",
        "train_with_l1(model_l1, optimizer_l1, lambda_l1=0.001)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_l1 = evaluate(model_l1, test_loader)\n",
        "print(f'MLP with L1 Regularization Accuracy: {accuracy_l1:.2f}%')\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
