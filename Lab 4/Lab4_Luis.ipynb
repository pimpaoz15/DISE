{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pLkutVcgVC76"
      },
      "source": [
        "# Lab 4\n",
        "Let's go back to using the Lab1 data and see if we are now able to improve on what linear regression was not able to do."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "f15Mf_sgUpqe"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "sns.set(rc={\"figure.figsize\": (10, 6)})\n",
        "import matplotlib.pyplot as plt\n",
        "import scipy.stats as stats\n",
        "\n",
        "plt.style.use('ggplot') # setting the plot style\n",
        "%matplotlib inline\n",
        "from __future__ import print_function, unicode_literals, division\n",
        "\n",
        "# ignore various warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n3Zyh_cLcUra",
        "outputId": "53962abc-023c-4919-b0b8-7b9be85188dc"
      },
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'google.colab'",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#import drive\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mgoogle\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcolab\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m drive\n\u001b[1;32m      4\u001b[0m drive\u001b[38;5;241m.\u001b[39mmount(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m/content/drive\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'google.colab'"
          ]
        }
      ],
      "source": [
        "#import drive\n",
        "\n",
        "# from google.colab import drive\n",
        "# drive.mount('/content/drive')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "P4Fh5u-_VPkm"
      },
      "outputs": [],
      "source": [
        "# planetData = pd.read_csv(\"/content/drive/MyDrive/Data-intensive/oec.csv\")\n",
        "\n",
        "planetData = pd.read_csv(\"oec.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4X1oszNc0If",
        "outputId": "e9889909-4bc9-4c43-c46e-9ec643191594"
      },
      "outputs": [],
      "source": [
        "planetData.columns"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AnIiadU8VhsW",
        "outputId": "aa79febc-8829-4115-f47b-1607f68b2360"
      },
      "outputs": [],
      "source": [
        "planetData['PeriodYears'] = planetData['PeriodDays']/365.25\n",
        "mult_cols = ['SemiMajorAxisAU','HostStarMassSlrMass',\n",
        "             'PlanetaryMassJpt','PeriodYears']   # take X to be 6 columns and y to be 1 column\n",
        "mult_features = ['SemiMajorAxisAU','HostStarMassSlrMass', 'PlanetaryMassJpt']\n",
        "planets_selectedFeatures = planetData[mult_cols].dropna()\n",
        "\n",
        "X_mult = planets_selectedFeatures[mult_features]\n",
        "y_mult = planets_selectedFeatures['PeriodYears']   # y is the PeriodYears column\n",
        "print(\"Number of observations: \", X_mult.shape[0])\n",
        "print(\"Number of values for the response variable y: \", y_mult.shape[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CGtpdCqMVigV",
        "outputId": "b45cc78e-b131-4669-bf1c-b7d7de1979ef"
      },
      "outputs": [],
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_mult, y_mult, test_size=0.3, random_state=42)\n",
        "# Split arrays or matrices into random train and test subsets\n",
        "# -- use random state for debugging purposes\n",
        "print(\"training data size:\",X_train.shape)\n",
        "print(\"testing data size:\",X_test.shape)\n",
        "print (\"total data size:\", X_mult.shape)\n",
        "print(\"size of y_mult:\", y_mult.shape)\n",
        "print(\"size of y_mult_train: \", y_train.shape)\n",
        "print(\"size of y_mult_test: \", y_test.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "YRzK-9DyVsCg"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Ed-pgI2VtMG"
      },
      "source": [
        "# Based on the previous examples, test a neural network to predict y_test and calculate r2.\n",
        "\n",
        "*  Try different configurations of layers and neurons and get a good result.\n",
        "*  With your final neural network configuration. Experiment with different learning rates (0.1, 0.01, 0.001) and batch sizes (8, 16, 32, 64). How do these hyperparameters affect the training process, loss, and final model accuracy? Run experiments by varying the learning rate and batch size, then compare the results in terms of convergence, accuracy, and training speed."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tq8Fj0HoqdfO"
      },
      "source": [
        "**STEP 1 - TRY DIFFERENT CONFIGURATIONS**\n",
        "\n",
        "**Layer Configurations**: The variable layer_configs defines different network architectures:\n",
        "\n",
        "*   [64]: A single hidden layer with 64 neurons.\n",
        "*   [128, 64]: Two hidden layers with 128 neurons in the first hidden layer and 64 in the second.\n",
        "*   [128, 64, 32]: Three hidden layers with 128, 64, and 32 neurons, respectively.\n",
        "\n",
        "**Learning Rates and Batch Sizes:** The network is trained using various learning rates (0.1, 0.01, 0.001) and batch sizes (8, 16, 32, 64) to experiment with the effect of these hyperparameters on training time, accuracy, and loss.\n",
        "\n",
        "**Dynamic Layer Construction**: The architecture of the network is dynamically created based on the number of layers and neurons specified in the layer_config variable.\n",
        "\n",
        "**Training Loop**: The training loop runs for a fixed number of epochs (100), and training/validation losses are printed every 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wG3A87NtW0cj",
        "outputId": "4fe7a80a-265e-4411-84b3-3e3533fb4cf8"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Define the neural network class with flexible architecture\n",
        "class PlanetNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(PlanetNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Create hidden layers dynamically\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(input_size, hidden_sizes[i]))  # First layer connects input to first hidden layer\n",
        "            else:\n",
        "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))  # Hidden layer to hidden layer\n",
        "\n",
        "            layers.append(nn.ReLU())  # Add ReLU activation after each hidden layer\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))  # Output layer\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Function to train the neural network\n",
        "def train_model(model, criterion, optimizer, train_loader, val_loader, epochs=100):\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Gradient descent\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                val_outputs = model(batch_x)\n",
        "                val_loss += criterion(val_outputs, batch_y).item()\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}/{epochs}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n",
        "# Function to evaluate the model and calculate R²\n",
        "def evaluate_model(model, X_test_tensor, y_test_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test_tensor)\n",
        "        r2 = r2_score(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
        "        print(f'R² Score: {r2:.4f}')\n",
        "        return r2\n",
        "\n",
        "# Convert the training and testing data to tensors\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "# Hyperparameter settings\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "batch_sizes = [8, 16, 32, 64]\n",
        "layer_configs = [\n",
        "    [64],        # 1 hidden layer with 64 neurons\n",
        "    [128, 64],   # 2 hidden layers with 128 and 64 neurons\n",
        "    [128, 64, 32]  # 3 hidden layers with 128, 64, and 32 neurons\n",
        "]\n",
        "\n",
        "for layer_config in layer_configs:\n",
        "    for lr in learning_rates:\n",
        "        for batch_size in batch_sizes:\n",
        "            print(f\"\\nTraining with layers {layer_config}, learning rate {lr}, and batch size {batch_size}...\\n\")\n",
        "\n",
        "            # Create DataLoader for training and validation\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor),\n",
        "                batch_size=batch_size, shuffle=True\n",
        "            )\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor),\n",
        "                batch_size=batch_size, shuffle=False\n",
        "            )\n",
        "\n",
        "            # Initialize the model, loss function, and optimizer\n",
        "            model = PlanetNet(input_size=3, hidden_sizes=layer_config, output_size=1)\n",
        "            criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model\n",
        "            train_model(model, criterion, optimizer, train_loader, val_loader, epochs=100)\n",
        "\n",
        "            # Evaluate the model\n",
        "            evaluate_model(model, X_test_tensor, y_test_tensor)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Uh97w93proQC"
      },
      "source": [
        "**STEP 2- Experiment with the best architecture ([128, 64, 32]) and then vary:**\n",
        "\n",
        "**Learning rates**: 0.1, 0.01, and 0.001\n",
        "**Batch sizes**: 8, 16, 32, and 64\n",
        "Graph and visualize the training loss, validation loss, and R² score for each combination of learning rate and batch size.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "7JVm_gZArupy",
        "outputId": "9b03194c-cd6a-46c7-c1f9-ed6ae40274e3"
      },
      "outputs": [],
      "source": [
        "# Import the necessary libraries\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import matplotlib.pyplot as plt\n",
        "import pandas as pd\n",
        "from sklearn.metrics import r2_score\n",
        "\n",
        "# Define the neural network class with the best architecture\n",
        "class BestPlanetNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_sizes, output_size):\n",
        "        super(BestPlanetNet, self).__init__()\n",
        "\n",
        "        layers = []\n",
        "        # Create hidden layers dynamically\n",
        "        for i in range(len(hidden_sizes)):\n",
        "            if i == 0:\n",
        "                layers.append(nn.Linear(input_size, hidden_sizes[i]))  # First layer connects input to first hidden layer\n",
        "            else:\n",
        "                layers.append(nn.Linear(hidden_sizes[i-1], hidden_sizes[i]))  # Hidden layer to hidden layer\n",
        "\n",
        "            layers.append(nn.ReLU())  # Add ReLU activation after each hidden layer\n",
        "\n",
        "        layers.append(nn.Linear(hidden_sizes[-1], output_size))  # Output layer\n",
        "        self.model = nn.Sequential(*layers)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Function to train the neural network\n",
        "def train_best_model(model, criterion, optimizer, train_loader, val_loader, epochs=100):\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    for epoch in range(epochs):\n",
        "        model.train()  # Set the model to training mode\n",
        "        train_loss = 0.0\n",
        "        for batch_x, batch_y in train_loader:\n",
        "            optimizer.zero_grad()  # Clear gradients\n",
        "            outputs = model(batch_x)\n",
        "            loss = criterion(outputs, batch_y)\n",
        "            loss.backward()  # Backpropagation\n",
        "            optimizer.step()  # Gradient descent\n",
        "            train_loss += loss.item()\n",
        "\n",
        "        # Validation loop\n",
        "        model.eval()  # Set the model to evaluation mode\n",
        "        val_loss = 0.0\n",
        "        with torch.no_grad():\n",
        "            for batch_x, batch_y in val_loader:\n",
        "                val_outputs = model(batch_x)\n",
        "                val_loss += criterion(val_outputs, batch_y).item()\n",
        "\n",
        "        # Store losses for visualization\n",
        "        train_losses.append(train_loss / len(train_loader))\n",
        "        val_losses.append(val_loss / len(val_loader))\n",
        "\n",
        "        if epoch % 10 == 0:\n",
        "            print(f'Epoch {epoch}/{epochs}, Train Loss: {train_loss / len(train_loader)}, Val Loss: {val_loss / len(val_loader)}')\n",
        "\n",
        "    return train_losses, val_losses\n",
        "\n",
        "# Function to evaluate the model and calculate R²\n",
        "def evaluate_best_model(model, X_test_tensor, y_test_tensor):\n",
        "    model.eval()\n",
        "    with torch.no_grad():\n",
        "        predictions = model(X_test_tensor)\n",
        "        r2 = r2_score(y_test_tensor.cpu().numpy(), predictions.cpu().numpy())\n",
        "        print(f'R² Score: {r2:.4f}')\n",
        "        return predictions, r2\n",
        "\n",
        "# Function to plot training and validation losses\n",
        "def plot_losses(train_losses, val_losses, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.plot(train_losses, label='Training Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title(title)\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to plot true vs predicted values\n",
        "def plot_true_vs_predicted(y_true, y_pred, title):\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.scatter(y_true, y_pred, alpha=0.5, color='blue')\n",
        "    plt.plot([y_true.min(), y_true.max()], [y_true.min(), y_true.max()], 'r--', lw=2)  # Reference line for perfect predictions\n",
        "    plt.xlabel('True Values')\n",
        "    plt.ylabel('Predicted Values')\n",
        "    plt.title(title)\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "\n",
        "# Function to conduct the full experiment\n",
        "def run_experiment(hidden_sizes, learning_rates, batch_sizes, epochs):\n",
        "    # Convert the training and testing data to tensors\n",
        "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "    y_train_tensor = torch.tensor(y_train.values, dtype=torch.float32).view(-1, 1)\n",
        "    y_test_tensor = torch.tensor(y_test.values, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "    # Loop over learning rates and batch sizes\n",
        "    for lr in learning_rates:\n",
        "        for batch_size in batch_sizes:\n",
        "            print(f\"\\nTraining with learning rate {lr} and batch size {batch_size}...\\n\")\n",
        "\n",
        "            # Create DataLoader for training and validation\n",
        "            train_loader = torch.utils.data.DataLoader(\n",
        "                torch.utils.data.TensorDataset(X_train_tensor, y_train_tensor),\n",
        "                batch_size=batch_size, shuffle=True\n",
        "            )\n",
        "            val_loader = torch.utils.data.DataLoader(\n",
        "                torch.utils.data.TensorDataset(X_test_tensor, y_test_tensor),\n",
        "                batch_size=batch_size, shuffle=False\n",
        "            )\n",
        "\n",
        "            # Initialize the model, loss function, and optimizer\n",
        "            model = BestPlanetNet(input_size=3, hidden_sizes=hidden_sizes, output_size=1)\n",
        "            criterion = nn.MSELoss()  # Mean Squared Error for regression\n",
        "            optimizer = optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "            # Train the model\n",
        "            train_losses, val_losses = train_best_model(model, criterion, optimizer, train_loader, val_loader, epochs=epochs)\n",
        "\n",
        "            # Evaluate the model\n",
        "            predictions, r2 = evaluate_best_model(model, X_test_tensor, y_test_tensor)\n",
        "\n",
        "            # Plot losses and true vs predicted values\n",
        "            plot_losses(train_losses, val_losses, f\"Loss: LR={lr}, Batch Size={batch_size}\")\n",
        "            plot_true_vs_predicted(y_test_tensor.cpu().numpy(), predictions.cpu().numpy(), f\"True vs Predicted: LR={lr}, Batch Size={batch_size}\")\n",
        "\n",
        "# Hyperparameters for the experiment\n",
        "hidden_sizes = [128, 64, 32]  # Best architecture\n",
        "learning_rates = [0.1, 0.01, 0.001]\n",
        "batch_sizes = [8, 16, 32, 64]\n",
        "epochs = 100\n",
        "\n",
        "# Run the experiment\n",
        "run_experiment(hidden_sizes, learning_rates, batch_sizes, epochs)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
